{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchain-openai langchain-experimental neo4j wikipedia tiktoken yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "os.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"password\"\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomazbratanic/anaconda3/envs/python39/lib/python3.9/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/tomazbratanic/anaconda3/envs/python39/lib/python3.9/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Read the wikipedia article\n",
    "raw_documents = WikipediaLoader(query=\"Ada Lovelace\").load()\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "documents = text_splitter.split_documents(raw_documents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\")\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2024-03-11 17:36:24.517242\n",
      "Finish at 2024-03-11 17:38:46.181223\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "print(f\"Start at {datetime.now()}\")\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "graph.add_graph_documents(graph_documents, baseEntityLabel=True, include_source=True)\n",
    "print(f\"Finish at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "# directly show the graph resulting from the given Cypher query\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(uri = os.environ[\"NEO4J_URI\"], auth = (os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb2776d3f544282a31cb9f8c9a8365b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomazbratanic/neo4j/langchain/libs/core/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Tomaz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Optional\n",
    "# Retriever\n",
    "\n",
    "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "entity_chain.invoke({\"question\": \"Hey, how is Tomaz doing?\"}).names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_lucene_chars(text: str) -> str:\n",
    "    \"\"\"Remove Lucene special characters\"\"\"\n",
    "    special_chars = [\n",
    "        \"+\",\n",
    "        \"-\",\n",
    "        \"&\",\n",
    "        \"|\",\n",
    "        \"!\",\n",
    "        \"(\",\n",
    "        \")\",\n",
    "        \"{\",\n",
    "        \"}\",\n",
    "        \"[\",\n",
    "        \"]\",\n",
    "        \"^\",\n",
    "        '\"',\n",
    "        \"~\",\n",
    "        \"*\",\n",
    "        \"?\",\n",
    "        \":\",\n",
    "        \"\\\\\",\n",
    "    ]\n",
    "    for char in special_chars:\n",
    "        if char in text:\n",
    "            text = text.replace(char, \" \")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~0.8) to each word, then combines them using the AND\n",
    "    operator. Useful for mapping movies and people from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~0.8 AND\"\n",
    "    full_text_query += f\" {words[-1]}~0.8\"\n",
    "    return full_text_query.strip()\n",
    "    \n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' - ' + neighbor.id AS output\n",
    "              UNION\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' - ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augusta Ada King - HOLDS_TITLE - Countess Of Lovelace\n",
      "Augusta Ada King - FRIEND - Charles Babbage\n",
      "Augusta Ada King - INTERESTED_IN - Analytical Engine\n",
      "Lord Byron - PARENT - Augusta Ada King\n",
      "Anne Isabella Milbanke - PARENT - Augusta Ada King\n",
      "William King - MARRIED - Augusta Ada King\n",
      "William King - HOLDS_TITLE - Earl Of Lovelace\n",
      "Augusta Ada King - MENTORED_BY - Mary Somerville\n",
      "Luigi Menabrea - HOLDS_TITLE - Prime Minister Of Italy\n",
      "Augusta Ada King - AUTHOR - Notes\n",
      "First Computer Program - CREATED_BY - Ada\n",
      "Babbage'S Personal Notes - DATE - 1836/1837\n",
      "Ada - EXAMINED - Analytical Engine\n",
      "Ada - ADVOCATED - Poetical Science\n",
      "Lord Byron - PARENT_OF - Ada\n",
      "Lady Byron - PARENT_OF - Ada\n",
      "Ada - NAMED_AFTER - Augusta Leigh\n",
      "Ada - MOVED_TO - Kirkby Mallory\n",
      "Lord Byron - SUBJECT_TO - English Law\n",
      "Lord Byron - DIED_IN - 1824\n",
      "Ada Lovelace - DEVELOPED_BY - Nvidia\n",
      "Ada Lovelace - SUCCESSOR_OF - Ampere Architecture\n",
      "Ada Lovelace - ANNOUNCED_ON - September 20, 2022\n",
      "Ada Lovelace - NAMED_AFTER - English Mathematician Ada Lovelace\n",
      "Nvidia - ANNOUNCED - Geforce 40 Series\n",
      "Nvidia - ANNOUNCED - Rtx 6000 Ada Generation\n",
      "Ada Lovelace - USES_TECHNOLOGY_FROM - Tsmc\n",
      "Ada Lovelace - USES - 5 Nm 4N Process\n",
      "Ada Lovelace - PREVIOUSLY_USED - Samsung 8 Nm Process\n",
      "Ada Lovelace - PREVIOUSLY_USED - Tsmc N7 Process\n",
      "Ada Lovelace - ANNOUNCED_BY - Jensen Huang\n",
      "Ada Lovelace - ANNOUNCED_AT - Gtc 2022\n",
      "Lovelace - CONTAINS - Tensor Cores\n",
      "Tensor Cores - ENABLES - Dlss 3\n",
      "Ampere - CONTAINS - Sm\n",
      "Lovelace - CONTAINS - Sm\n",
      "Ada Lovelace Architecture - INCLUDES - Rtx 4090\n",
      "Rtx 4090 - SUPERSEDES - Rtx 3090 Ti\n",
      "Ad102 Lovelace Die - FEATURES - L2 Cache\n",
      "Ga102 Die - CONTAINED_IN - L2 Cache\n",
      "L2 Cache - PREFERS_OVER - Gddr Video Memory\n",
      "Ada Lovelace Architecture - USES - Memory Controller\n",
      "Ada Lovelace Architecture - SUPPORTS - Gddr6\n",
      "Ada Lovelace Architecture - SUPPORTS - Gddr6X\n",
      "Geforce Rtx 40 Series - USES - Gddr6X\n",
      "Rtx A6000 - USES - Gddr6\n",
      "Ada Lovelace Architecture - PARTNERED_WITH - Tsmc\n",
      "Tsmc - DEVELOPED - 4N Process\n",
      "Ada Lovelace Day - NAMED_AFTER - Ada Lovelace\n",
      "Ada Lovelace Day - CELEBRATES - Stem\n"
     ]
    }
   ],
   "source": [
    "print(structured_retriever(\"How is Lovelace doing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question: str):\n",
    "    print(question)\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from typing import Tuple\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Ada Lovelace?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ada Lovelace, born Augusta Ada King, Countess of Lovelace (n√©e Byron), was an English mathematician and writer, most notably recognized for her work on Charles Babbage\\'s proposed mechanical general-purpose computer, the Analytical Engine. She is celebrated for being the first to recognize that the machine had potential applications beyond pure calculation, effectively making her one of the first computer programmers. Ada Lovelace was the only legitimate child of the poet Lord Byron and his wife, Anne Isabella Milbanke. Her contributions to the field of computing are commemorated by Ada Lovelace Day, an annual event celebrating the achievements of women in STEM fields. Additionally, Nvidia named a graphics processing unit microarchitecture \"Lovelace\" in her honor, highlighting her lasting impact on the fields of mathematics and computer science.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Who is Ada Lovelace?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where did Ada Lovelace live?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ada Lovelace moved to Kirkby Mallory.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Where did she live?\", \"chat_history\": [(\"Who is Ada Lovelace?\", \"She is a cool person from the past.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
